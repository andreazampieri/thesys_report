\documentclass{article}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{scrextend}
\makeatletter
\let\@addmarginORIG\@addmargin
\renewcommand*\@addmargin{%
	\vspace{-\bigskipamount}
	\@addmarginORIG}
\makeatother

\author{Andrea Zampieri}
\title{Network Inference from Node Embedding:\newline Deep Autoencoding}


\begin{document}
	\maketitle
	\newpage
	\section{Introduction} 
		I started from the Zekarias	T. Kefato's paper: \textit{DeepInfer: Diffusion Network Inference through Representation Learning}.\\
		The problem tackled in this paper (as described by its title) is inferring a network starting from series of interactions between the elements in play.
		The goal is to obtain a reconstruction as precise as possible of the actual \textbf{Interaction Network} without using any kind of exact information on the actual graph.\\ 
		\\
		\textbf{Note:} for simplicity and immediacy sake, during the explanation I will make references to a specific example in order to show an instance of the problem:\\
		
		\begin{addmargin}[2em]{0em}
			\textbf{\textit{Twitter}}: the interactions observed are formed by:\\
				\begin{addmargin}[2em]{0em}
					- \textit{contagious element}: the ones taken in consideration are the \textit{hashtags} \\
					- \textit{infection spreading}: each contagiant has a list of user infected (with the timestamp). This indicates which users used that hashtag.
				\end{addmargin}
		\end{addmargin}
	
	\section{Reproduction of the SoA Tecnique}
		\subsection{Brief Overview}
			The State of Art technique has precise, consequential steps:
			\begin{itemize}
				\item Cascade Linearization
				\item Node embedding
				\item Inference evaluation
			\end{itemize}
		\subsection{Cascade Linearization}
			The desired input formatting is a list of sequences of interaction, the contagiant element and the time between each infection isn't relevant. This kind of lists are called cascades.\\
			In order to obtain this kind of representation - for each hashtag cascade - it's necessary to sort each user for ascending timestamp, and once did so, stripe off the time values, since aren't required for the computations.\\
			\paragraph{Example}
				\begin{addmargin}[1em]{0em}
					$$c_{j}: [(u_{1},t_{1}), ..., (u_{n},t_{n})]$$
					\\
					where $u_{i}$ is the id of the user and $t_{i}$ its timestamp relative to the cascade $c_{j}$ represented by its hashtag.\\
					The representation becomes: $[u_{k},...,u_{l}]$ where the users $u$ are sorted by ascending timestamp. \textbf{Note:} the hashtag and the timestamps are stripped off because they aren't relevant to the goal of network inference.
				\end{addmargin}
		\subsection{Node Embedding}
			\textit{Node Embedding} states for a different representation of the nodes of the graph (in our case, the users).\\
			In our case it's done via \textit{n-dimensional} arrays of floating-point values.\\
			This method is largely used for context prediction of words based on some reference documents. The leading idea is to assign similar set of values to words that occur within the same context (a determined value of width of the proximity window).\\
			\paragraph{Example}
			\begin{addmargin}[1em]{0em}
	            \medskip
				\tab Document: \textit{\textquotedblleft The quick brown fox jumps over the lazy dog\textquotedblright}\smallskip \\
				With a context window of 2, the words that would be considered into the context, with respect to \textit{\textbf{fox}} would be \textit{quick, brown, jumps} and \textit{over} because they are within a range of 2 words from fox.
			\end{addmargin}
			The values are assigned by assuming that the probability of observing the words in the document-order of each document is the maximum.\\
			The same kind of reasoning is done in our case:
			\begin{itemize}
				\item [-] The documents are represented by the cascades
				\item [-] Users in the cascades are the equivalent of the words
			\end{itemize}
			The algorithm used is \textit{Word2Vec}; it's based on a Skip-gram model, i.e. the training samples are pairs of words that are in the same context window even if they aren't adjacent. (E.g. using the previous sample phrase, the pairs generated from the word fox, would be (quick,fox),(brown,fox),(fox,jumps),(fox,over)).\\
			The NN trained has 3 layers:
			\begin{itemize}
				\item [-] \textit{input layer}: layer of size V [size of the vocabulary]. Each input neuron corresponds to a different word among the whole set of the latters
				\item [-] \textit{encoding layer}: layer of size N [size of the embedding, N-dimensional space representation]. It's the layer that will construct the representation for each word.
				\item [-] \textit{output layer}: layer of size V. As explained before for the input layer, each neuron maps a word.
			\end{itemize}
			The training is done with a softmax function that for each pair of words (context,target) maximizes the probability of having predicted the \textit{target} given the \textit{context}.\medskip\\
			Usually the dimension of the embedding (the n-valued representation of the nodes) is of magnitude at most 2 (depends on the magnitude of distinct values present among all the cascades).\\
			Note that a 100-dimensional-space-representation for each node in a very large graph is a far more compressed representation than it's adjacency list.\medskip\\
			The nodes of the graph are treated like the words in the documents and the cascades like the documents.
		\subsection{Inference Evaluation}
			\paragraph{}
			The network inference is executed using the representation of each node. The prediction about whether an edge is present between two nodes $u$ and $v$ is based on the \textbf{cosine similarity} of the two representation.
			$$ Sim(u,v) = \frac{|u\cdot v|}{|u|\cdot |v|} $$
			where $|n|$ is the norm of the array-representation of the n and $|u|\cdot |v|$ is the cross product of the two representation.\\
			The inference is based on the parameter $\theta$ that determines the threshold of acceptance. If $Sim(u,v) \ge \theta$ the arc is predicted present, not present otherwise.\\
			The first way of evaluating is to generate a certain number of edges and asserting whether the prediction was correct or not; it's relevant to the evaluation goal to analyse every case (i.e. true positives, false positives, true negatives and true negatives).\\ 
			
\end{document}