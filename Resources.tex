\documentclass{article}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{makeidx}
\graphicspath{ {images/} }
\usepackage{scrextend}
\makeatletter
\let\@addmarginORIG\@addmargin
\renewcommand*\@addmargin{%
	\vspace{-\bigskipamount}
	\@addmarginORIG}
\makeatother


\newenvironment{tightcenter}{%
	\setlength\topsep{0pt}
	\setlength\parskip{0pt}
	\begin{center}
	}{%
\end{center}
}

\author{Andrea Zampieri}
\title{Network Inference from Node Embedding:\newline Deep Autoencoding}


\begin{document}
	\pagestyle{plain}
	
	\thispagestyle{empty}
	
	\begin{center}
		\begin{figure}[h!]
			\centerline{\psfig{file=logo_unitn_black_center.eps, width=0.6\textwidth}}
		\end{figure}
		
		\vspace{0 cm} 
		
		\LARGE{Dipartimento di Ingegneria e Scienza dell’Informazione\\}
		
		\vspace{1 cm} 
		\Large{Corso di Laurea in\\
			Informatica
		}
		
		%\vspace{2 cm} 
		%\Large\textsc{Elaborato finale\\} 
		\vspace{1 cm} 
		\Huge\textsc{Network Inference via  Node Embedding\\}
		
		\vspace{2 cm} 
		\begin{tabular*}{\textwidth}{ c @{\extracolsep{\fill}} c }
			\Large{Relatore} & \Large{Laureando}\\
			\Large{Alberto Montresor}& \Large{Andrea Zampieri}\\
		\end{tabular*}
		
		\vspace{2 cm} 
		
		\Large{Anno accademico 2016/2017}
		
	\end{center}
	
	
	\newpage
	\tableofcontents{}
	\newpage
	\section{Introduction} 
		I started from the Zekarias	T. Kefato's paper: \textit{DeepInfer: Diffusion Network Inference through Representation Learning}.\\
		The problem tackled in this paper (as described by its title) is inferring a network starting from series of interactions between the elements in play.
		The goal is to obtain a reconstruction as precise as possible of the actual \textbf{Interaction Network} without using any kind of exact information on the actual graph.\\ 
		\\
		\textbf{Note:} for simplicity and immediacy sake, during the explanation I will make references to a specific example in order to show an instance of the problem:\\
		
		\begin{addmargin}[2em]{0em}
			\textbf{\textit{Twitter}}: the interactions observed are formed by:\\
				\begin{addmargin}[2em]{0em}
					- \textit{contagious element}: the ones taken in consideration are the \textit{hashtags} \\
					- \textit{infection spreading}: each contagiant has a list of user infected (with the timestamp). This indicates which users used that hashtag.
				\end{addmargin}
		\end{addmargin}
	
	\section{Reproduction of the SoA Tecnique}
		\subsection{Brief Overview}
			The State of Art technique has precise, consequential steps:
			\begin{itemize}
				\item Cascade Linearization
				\item Node embedding
				\item Inference evaluation
			\end{itemize}
		\subsection{Cascade Linearization}
			The desired input formatting is a list of sequences of interaction, the contagiant element and the time between each infection isn't relevant. This kind of lists are called cascades.\\
			In order to obtain this kind of representation - for each hashtag cascade - it's necessary to sort each user for ascending timestamp, and once did so, stripe off the time values, since aren't required for the computations.\\
			\paragraph{Example}
				\begin{addmargin}[1em]{0em}
					$$c_{j}: [(u_{1},t_{1}), ..., (u_{n},t_{n})]$$
					\\
					where $u_{i}$ is the id of the user and $t_{i}$ its timestamp relative to the cascade $c_{j}$ represented by its hashtag.\\
					The representation becomes: $[u_{k},...,u_{l}]$ where the users $u$ are sorted by ascending timestamp. \textbf{Note:} the hashtag and the timestamps are stripped off because they aren't relevant to the goal of network inference.
				\end{addmargin}
		\subsection{Node Embedding}
			\textit{Node Embedding} states for a different representation of the nodes of the graph (in our case, the users).\\
			In our case it's done via \textit{n-dimensional} arrays of floating-point values.\\
			This method is largely used for context prediction of words based on some reference documents. The leading idea is to assign similar set of values to words that occur within the same context (a determined value of width of the proximity window).\\
			\paragraph{Example}
			\begin{addmargin}[1em]{0em}
	            \medskip
				\tab Document: \textit{\textquotedblleft The quick brown fox jumps over the lazy dog\textquotedblright}\smallskip \\
				With a context window of 2, the words that would be considered into the context, with respect to \textit{\textbf{fox}} would be \textit{quick, brown, jumps} and \textit{over} because they are within a range of 2 words from fox.
			\end{addmargin}
			The values are assigned by assuming that the probability of observing the words in the document-order of each document is the maximum.\\
			The same kind of reasoning is done in our case:
			\begin{itemize}
				\item [-] The documents are represented by the cascades
				\item [-] Users in the cascades are the equivalent of the words
			\end{itemize}
			The algorithm used is \textit{Word2Vec}; it's based on a Skip-gram model, i.e. the training samples are pairs of words that are in the same context window even if they aren't adjacent. (E.g. using the previous sample phrase, the pairs generated from the word fox, would be (quick,fox),(brown,fox),(fox,jumps),(fox,over)).\\
			The NN trained has 3 layers:
			\begin{itemize}
				\item [-] \textit{input layer}: layer of size V [size of the vocabulary]. Each input neuron corresponds to a different word among the whole set of the latters
				\item [-] \textit{encoding layer}: layer of size N [size of the embedding, N-dimensional space representation]. It's the layer that will construct the representation for each word.
				\item [-] \textit{output layer}: layer of size V. As explained before for the input layer, each neuron maps a word.
			\end{itemize}
			The training is done with a softmax function that for each pair of words (context,target) maximizes the probability of having predicted the \textit{target} given the \textit{context}.\medskip\\
			Usually the dimension of the embedding (the n-valued representation of the nodes) is of magnitude at most 2 (depends on the magnitude of distinct values present among all the cascades).\\
			Note that a 100-dimensional-space-representation for each node in a very large graph is a far more compressed representation than it's adjacency list.\medskip\\
			The nodes of the graph are treated like the words in the documents and the cascades like the documents.
		\subsection{Inference Evaluation}
			\paragraph{}
			The network inference is executed using the representation of each node. The prediction about whether an edge is present between two nodes $u$ and $v$ is based on the \textbf{cosine similarity} of the two representation.
			$$ Sim(u,v) = \frac{|u\cdot v|}{|u|\cdot |v|} $$
			where $|n|$ is the norm of the array-representation of the n and $|u|\cdot |v|$ is the cross product of the two representation.\\
			The inference is based on the parameter $\theta$ that determines the threshold of acceptance. If $Sim(u,v) \ge \theta$ the arc is predicted present, not present otherwise.\\
			The first way of evaluating is to generate a certain number of edges and asserting whether the prediction was correct or not; it's relevant to the evaluation goal to analyse every case (i.e. true positives, false positives, true negatives and true negatives).\medskip\\

			With respect to the use of the cascades, in order to increase the performances not the whole dataset is utilised. As said in DI, shorter cascades contains more relevant informations about the communities in the graph; observing viral \textquotedblleft contaminations" tells us [TBC] basically nothing about the relation between the node of the graph. \\
			\begin{quote}
				\centering
				\textsc{missing joining phrases}\\
			\end{quote}
			\subsubsection{Pairs co-occurrence}
			Computing the co-occurrence of pairs of nodes from a set of cascades consists in fetching all the cascades, and for each one of them increasing the counter of the pair $(u,v)$ if $u$ occurs before $v$ in the cascade.\\
			This information will be useful to create a probabilistic adjacency list representation for the Deep Autoencoder.
			\paragraph{Goodness of the pairs}
			Running the evaluation process on the pairs of nodes obtained by filtering too-long-cascades, computing the co-occurrence count and sorted by the latter parameter, there is a high percentage of true positives, with low ratio of false positives and true negatives. This gives us the confidence to use a representation for each node $u$ that resembles an adjacency-list that contains 0 at the index $v$ where the pair $(u,v)$ hasn't appeared, in the other case contains the counter.\\
			\begin{quote}
				\centering
				\textsc{To be finished}\\
			\end{quote}
			
		\section{Neural Network concepts}
			\subsection{Deep Autoencoder}
			Deep Autoencoders (AE) are deep feedforward neural networks with the goal of reproducing the input as accurately as possible. The architecture can be seen as two different parts: the encoder and the decoder. These parts are one after the other and the goal of the network can be summarized in this fashion:
			$$y=dec(enc(x))$$
			where $y$ is the output, $x$ the input and $enc()$ and $dec()$ the functions of encoding and decoding. This NN (Neural Network) tries to replicate the input to the output and by doing this, also finding a good representation of the input with $enc(x)$. The architecture of the two parts is mirrored.\\
			There are 2 types of autoencoder architectures: \textit{overcomplete} and \textit{undercomplete}.
			\paragraph{Overcomplete AE}
			An AE is \textit{overcomplete} when the encoding layer is larger in size wrt. the input and the output. This type of autoencoder is forced to learn a richer representation of the input, accomplishing a similar job to the extraction of latent variables and data-pattern recognition
			\paragraph{Undercomplete AE}
			In an \textit{undercomplete} AE the encoding layer is smaller than the input and output ones. With this setup, the NN tries to learn a compressed representation of the input data. In some way this can be seen as a process of feature extraction; moreover it isn't infrequent that AE performances are compared to PCA (Principal Component Analysis: statistical method for dimensionality reduction in information theory) in publications.
			\par \noindent \newline
			
		\section{Work}
			The work done has the goal to verify whether the use of a Deep AE to accomplish the embedding(the n-dimensional node representation) lead to an improvement in performances, wrt. the embedding created by Word2Vec-algorithm.
			\begin{quote}
				\centering
				\textsc{finish}
			\end{quote}
			
			\subsection{Inspiration}
			\textit{Structural Deep Network Embedding} (SDNE) is a paper that covers the idea of reducing the data saved for each node in a network but maintaining good efficacy in reconstructing the network.\\
			The approach described in this paper it's related to completely known graphs, which isn't our case. In fact SDNE, as starting representation for each node utilizes its adjacency list in vector form:
			$$ S_{k} = [s_{1}, ... , s_{n}] $$
			$$ s_{i} > 0 \iff (v_{k},v_{i})\in E $$\\
			with $E$ being the set of edges of the graph, $S_{k}$ the representation relative to the node $v_{k}$ and $S$ the input for the NN. This representation is \textquotedblleft compressed" by the Deep AutoEncoder extracting a lighter dataset that can be very useful when dealing with huge graphs.\\
			Since the state of the graph is completely unknown to us, the idea is to replicate the same process utilising a slightly different way to identify nodes:
			\begin{equation}
			\begin{aligned}
			& S_{k} = [s_{1}, ... , s_{n}]\\
			& s_{i} = count(v_{k},v_{i})\\
			& count(u,v) =
			\begin{cases}
			c & \text{if the pair } (u,v) \text{  appears } c \text{ times in the cascades}\\
			0 & \text{if it never appears}\\
			\end{cases}\\
			\end{aligned}
			\end{equation}
			This list of values resembles a traditional adjacency list, but it's completely inferred by the cascades without any assurance on the truthfulness of the guesses.\\
			
			\subsection{Note on cascades and pairs of nodes}
			As previously said, short cascades gives us more information and more accuracy in prediction.\\
			Taking in example the case of the Twitter dataset, cascades are list of users that used a certain hashtag; in a small cascade we observe few people using the same hashtag, and so we can guess with fairly high confidence that they must follow each other because they saw the hashtag from each other. On the other hand the information is very local and doesn't cover the situation about the interconnection between communities and/or less evident grades of connection.\medskip\\
			The threshold $\lambda$ of the length of the cascades involved into the process of learning becomes an hyperparameter of the procedure. An expected phenomenon bound to the variation of $\lambda$ is that the lower it is, the more accurate will be the predictions, but the fewer arcs will be possible to infer. On the contrary, the exact opposite happens as $\lambda$ increases.
			
			\begin{algorithm}[H] % enter the algorithm environment
				\caption{Generate the pairs of nodes starting from a set of cascades} % give the algorithm a caption
				\label{alg1} % and a label for \ref{} commands later in the document
				\begin{algorithmic} % enter the algorithmic environment
					\REQUIRE $C$ the set of cascades, $\lambda$ the length threshold
					\ENSURE $CD$ the co-occurrence dictionary
					\STATE $C \gets$ sort\_by\_length(C)
					\STATE $CD \gets dict(\text{key:} (int,int), \text{value:} int)$
					\FORALL{$c \text{ in } C \text{ such as } c.length() \le \lambda$}
						\FOR{$i$ from  $1$ to $c.length()$}
							\FOR{$j$ from $i+1$ to $c.length()$}
								\STATE update\_occurrence($CD,pair(c[i],c[j])$)
							\ENDFOR
						\ENDFOR
					\ENDFOR
				\end{algorithmic}
			\end{algorithm}
			
			\begin{algorithm}[H] % enter the algorithm environment
				\caption{Given a key, increases the value by one of a Dictionary} % give the algorithm a caption
				\label{alg2} % and a label for \ref{} commands later in the document
				\begin{algorithmic} % enter the algorithmic environment
					\REQUIRE $D$ the dictionary, $k$ the searched key
					\ENSURE the increment of the counter(the value) related to $k$
					\IF{$D.$lookup($k$) $\neq$ NULL}
						\STATE $D[k] \gets D[k]+1$
					\ELSE
						\STATE $D[k] \gets 1$
					\ENDIF
				\end{algorithmic}
			\end{algorithm}
			
			\subsection{Deep Autoencoder Training}
			The autoencoder learns to reproduce its input-data batches effectively, throughout different epochs of training specializing in compressing reproductions of the fixed dataset. Once the training is completed, the middle-layer of the neural network gives us the representation for a given input; this can be used as a substitute of the Word2Vec encoding for the network-inference task. The inference and evaluation process is done the same way it was executed in the paper DI.
			
				\subsubsection{Motivation}
				The main reason why this attempt of improvement has been tried is because of the different architecture that offers the embedding.\\
				In \textit{Word2Vec}, the model tries to enforce similar representation for words in the same context-window trough a shallow Autoencoder trained with the goal of predicting couples of words.\\
				But as explained in SDNE, it's hard to capture highly non-linear structure (in this case the structure of the network)
			
	\section{Bibliography}
	\begin{thebibliography}{9}
		
		\bibitem{deepinfer}
		Zekarias T. Kefato and Nasrullah Sheikh and Alberto Montresor, 2017, \textit{\textsc{DeepInfer}: Diffusion Network Inference through Representation Learning}, KDD2017, Halifax, Nova Scotia, Canada
		
		\bibitem{deeplearningbook}
		Ian Goodfellow and Yoshua Bengio and Aaron Courville, 2016, \textit{Deep Learning}, MIT Press, http://www.deeplearningbook.org
		
		\bibitem{sdne}
		Daixin Wang, Peng Cui and Wenwu Zhu, 2016, \textit{Structural Deep Network Embedding}, KDD ’16, San Francisco (CA), USA
		
	\end{thebibliography}
			
\end{document}